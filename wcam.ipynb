{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is to recognize faces on live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing MTCNN and InceptionResnetV1 \n",
    "\n",
    "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
    "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=40) # keep_all=True\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# Read data from folder\n",
    "\n",
    "dataset = datasets.ImageFolder('photos') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True) \n",
    "    if face is not None and prob>0.92:\n",
    "        emb = resnet(face.unsqueeze(0)) \n",
    "        embedding_list.append(emb.detach()) \n",
    "        name_list.append(idx_to_class[idx])        \n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] \n",
    "torch.save(data, 'data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001_0000255', '0001_0000255', '0001_0000255', '0001_0000262', '0001_0000262', '0001_0000262', '0001_0000264', '0001_0000264', '0001_0000264', '0001_0000265', '0001_0000265', '0001_0000265', '0001_0000268', '0001_0000268', '0001_0000268', '0001_0000268', '0001_0000274', '0001_0000274', '0001_0000274', '0001_0000274', '0001_0000274', '0001_0000278', '0001_0000278', '0001_0000281', '0001_0000281', '0001_0000283', '0001_0000283', '0001_0000283', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000284', '0001_0000286', '0001_0000286', '0001_0000292', '0001_0000292', '0001_0000292', '0001_0000292', '0001_0000292', '0001_0000293', '0001_0000293', '0001_0000293', '0001_0000297', '0001_0000297', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000298', '0001_0000299', '0001_0000299', '0001_0000301', '0001_0000301', '0001_0000301', '0001_0000301', '0001_0000303', '0001_0000303', '0001_0000304', '0001_0000304', '0001_0000305', '0001_0000305', 'biplob', 'hritik', 'karan', 'maha', 'rohan']\n",
      "0001_0000305 0.9999341\n",
      "0001_0000292 0.9997305\n",
      "0001_0000265 0.9997248\n",
      "0001_0000305 0.9992754\n",
      "0001_0000292 0.99933594\n",
      "0001_0000292 0.99962807\n",
      "0001_0000292 0.9994106\n",
      "0001_0000292 0.9990262\n",
      "0001_0000305 0.9978015\n",
      "0001_0000292 0.998396\n",
      "0001_0000292 0.99956757\n",
      "0001_0000292 0.9981781\n",
      "0001_0000265 0.9987268\n",
      "0001_0000265 0.99914837\n",
      "0001_0000292 0.9990994\n",
      "0001_0000292 0.9993656\n",
      "biplob 0.99802655\n",
      "0001_0000292 0.99852103\n",
      "0001_0000292 0.9991204\n",
      "0001_0000292 0.9997236\n",
      "0001_0000292 0.99991083\n",
      "0001_0000292 0.9996352\n",
      "0001_0000292 0.9999305\n",
      "0001_0000292 0.9999435\n",
      "0001_0000292 0.99992454\n",
      "0001_0000292 0.9998832\n",
      "0001_0000298 0.999819\n",
      "0001_0000298 0.9999218\n",
      "0001_0000292 0.99994254\n",
      "0001_0000292 0.9999292\n",
      "0001_0000292 0.9999238\n",
      "0001_0000292 0.9999342\n",
      "0001_0000292 0.99994123\n",
      "0001_0000292 0.9999405\n",
      "0001_0000292 0.99957544\n",
      "0001_0000292 0.9999466\n",
      "0001_0000292 0.9999169\n",
      "0001_0000305 0.9997663\n",
      "0001_0000292 0.9985788\n",
      "maha 0.9998926\n",
      "maha 0.9990344\n",
      "maha 0.9997662\n",
      "0001_0000293 0.9995484\n",
      "0001_0000293 0.99971133\n",
      "0001_0000293 0.99992335\n",
      "0001_0000265 0.99990463\n",
      "0001_0000298 0.999949\n",
      "0001_0000265 0.99979633\n",
      "0001_0000265 0.9998017\n",
      "0001_0000265 0.9995146\n",
      "0001_0000265 0.9995654\n",
      "0001_0000265 0.9994747\n",
      "0001_0000265 0.9996092\n",
      "0001_0000265 0.9994517\n",
      "0001_0000265 0.999453\n",
      "0001_0000265 0.9995939\n",
      "0001_0000265 0.99962306\n",
      "0001_0000265 0.999426\n",
      "0001_0000265 0.9995029\n",
      "0001_0000265 0.99943966\n",
      "0001_0000265 0.9995347\n",
      "0001_0000265 0.9995577\n",
      "0001_0000265 0.99888617\n",
      "0001_0000265 0.9994972\n",
      "0001_0000265 0.9997373\n",
      "0001_0000265 0.99977845\n",
      "0001_0000265 0.9997328\n",
      "0001_0000265 0.99973553\n",
      "0001_0000265 0.99978095\n",
      "0001_0000265 0.999835\n",
      "0001_0000265 0.9996766\n",
      "0001_0000265 0.9998367\n",
      "0001_0000265 0.9997272\n",
      "0001_0000265 0.9998658\n",
      "0001_0000265 0.99983966\n",
      "Esc pressed, closing...\n"
     ]
    }
   ],
   "source": [
    "# Using webcam recognize face\n",
    "\n",
    "# loading data.pt file\n",
    "load_data = torch.load('data.pt') \n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1] \n",
    "print(name_list)\n",
    "\n",
    "cam = cv2.VideoCapture(0) \n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"fail to grab frame, try again\")\n",
    "        break\n",
    "        \n",
    "    img = Image.fromarray(frame)\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "                \n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                print(name,prob )\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "                \n",
    "                \n",
    "\n",
    "                #if min_dist<0.30:\n",
    "                frame = cv2.putText(frame, name +' '+str(prob), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,0),2, cv2.LINE_AA)\n",
    "                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)   \n",
    "                \n",
    "    cv2.imshow(\"IMG\", frame)\n",
    "        \n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256==27: # ESC\n",
    "        print('Esc pressed, closing...')\n",
    "        break\n",
    "        \n",
    "    elif k%256==32: # space to save image\n",
    "        print('Enter your name :')\n",
    "        name = input()\n",
    "        \n",
    "        # create directory if not exists\n",
    "        if not os.path.exists('photos/'+name):\n",
    "            os.mkdir('photos/'+name)\n",
    "            \n",
    "        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(\" saved: {}\".format(img_name))\n",
    "        \n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
